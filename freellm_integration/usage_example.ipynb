{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreeLLM Integration - Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the FreeLLM client to interact with the apifreellm.com API.\n",
    "\n",
    "The client follows patterns similar to OpenAI's library and integrates seamlessly with the Agents SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from freellm_integration import FreeLLMClient, ChatResponse\n",
    "from freellm_integration.config import FreeLLMConfig\n",
    "from freellm_integration.utils import setup_logging, format_response_for_display\n",
    "from freellm_integration.exceptions import FreeLLMAPIError, FreeLLMConnectionError\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "import asyncio\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chat\n",
    "\n",
    "Send a simple message to the FreeLLM API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 00:55:03 - httpx - INFO - HTTP Request: POST https://apifreellm.com/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**AI Response:**\n",
       "\n",
       "Hello! As an AI, I don't have feelings or emotions, but I'm functioning well and ready to assist you with any questions or tasks you have. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def basic_chat_example():\n",
    "    \"\"\"Send a basic chat message.\"\"\"\n",
    "    async with FreeLLMClient() as client:\n",
    "        response = await client.chat(\"Hello AI! How are you today?\")\n",
    "        display(Markdown(f\"**AI Response:**\\n\\n{response.response}\"))\n",
    "        return response\n",
    "\n",
    "# Run the example\n",
    "response = await basic_chat_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Chat with Parameters\n",
    "\n",
    "Use custom parameters like temperature and max_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:03:22 - httpx - INFO - HTTP Request: POST https://apifreellm.com/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Creative Haiku:**\n",
       "\n",
       "Code awakens light,  \n",
       "Neural webs of endless thought,  \n",
       "Man and spark entwine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def chat_with_parameters():\n",
    "    \"\"\"Send a chat message with custom parameters.\"\"\"\n",
    "    async with FreeLLMClient() as client:\n",
    "        response = await client.chat(\n",
    "            message=\"Write a creative haiku about artificial intelligence\",\n",
    "            temperature=0.9,  # Higher temperature for more creativity\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        display(Markdown(f\"**Creative Haiku:**\\n\\n{response.response}\"))\n",
    "        \n",
    "        if response.usage:\n",
    "            print(f\"\\nToken usage: {response.usage}\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Run the example\n",
    "response = await chat_with_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Conversation with Context\n",
    "\n",
    "Have a multi-turn conversation where the AI remembers previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My name is Alice and I love Python programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:03:53 - httpx - INFO - HTTP Request: POST https://apifreellm.com/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** I'm Llama, a large language model developed by Meta. I'm here to help you with your questions and tasks. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "User: What's my name and what do I like?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:04:00 - httpx - INFO - HTTP Request: POST https://apifreellm.com/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**AI:** Your name is Alice, and you love Python programming. Want help with a Python project or topic?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Conversation has 4 messages\n"
     ]
    }
   ],
   "source": [
    "async def conversation_example():\n",
    "    \"\"\"Have a conversation with context.\"\"\"\n",
    "    async with FreeLLMClient() as client:\n",
    "        # First message\n",
    "        print(\"User: My name is Alice and I love Python programming\")\n",
    "        response1 = await client.chat_with_context(\n",
    "            \"My name is Alice and I love Python programming\"\n",
    "        )\n",
    "        display(Markdown(f\"**AI:** {response1.response}\"))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Second message (remembers context)\n",
    "        print(\"User: What's my name and what do I like?\")\n",
    "        response2 = await client.chat_with_context(\n",
    "            \"What's my name and what do I like?\"\n",
    "        )\n",
    "        display(Markdown(f\"**AI:** {response2.response}\"))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Show conversation history\n",
    "        history = client.get_history()\n",
    "        print(f\"Conversation has {len(history.messages)} messages\")\n",
    "        \n",
    "        return response2\n",
    "\n",
    "# Run the example\n",
    "response = await conversation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Error Handling\n",
    "\n",
    "Demonstrate proper error handling with custom exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def error_handling_example():\n",
    "    \"\"\"Demonstrate error handling.\"\"\"\n",
    "    # Example with very short timeout\n",
    "    client = FreeLLMClient(timeout=0.001)  # Extremely short timeout\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat(\"Hello!\")\n",
    "        print(response.response)\n",
    "    \n",
    "    except FreeLLMConnectionError as e:\n",
    "        print(f\"❌ Connection Error: {e}\")\n",
    "    \n",
    "    except FreeLLMAPIError as e:\n",
    "        print(f\"❌ API Error: {e}\")\n",
    "        if hasattr(e, 'status_code'):\n",
    "            print(f\"   Status Code: {e.status_code}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected Error: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        await client.close()\n",
    "        print(\"✓ Client closed successfully\")\n",
    "\n",
    "# Run the example\n",
    "await error_handling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Custom Configuration\n",
    "\n",
    "Use a custom configuration loaded from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def custom_config_example():\n",
    "    \"\"\"Use custom configuration.\"\"\"\n",
    "    # Load configuration from environment\n",
    "    config = FreeLLMConfig.from_env()\n",
    "    \n",
    "    print(\"Current Configuration:\")\n",
    "    print(f\"  Base URL: {config.base_url}\")\n",
    "    print(f\"  Timeout: {config.timeout}s\")\n",
    "    print(f\"  Max Retries: {config.max_retries}\")\n",
    "    print(f\"  Default Temperature: {config.default_temperature}\")\n",
    "    print(f\"  Max History: {config.max_conversation_history}\")\n",
    "    \n",
    "    # Create client with custom config\n",
    "    async with FreeLLMClient(\n",
    "        base_url=config.base_url,\n",
    "        timeout=config.timeout,\n",
    "        max_retries=config.max_retries,\n",
    "    ) as client:\n",
    "        response = await client.chat(\n",
    "            \"Explain what you are in one sentence\",\n",
    "            temperature=config.default_temperature,\n",
    "        )\n",
    "        display(Markdown(f\"**AI:** {response.response}\"))\n",
    "        return response\n",
    "\n",
    "# Run the example\n",
    "response = await custom_config_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Integration with Agents SDK\n",
    "\n",
    "Use FreeLLM client alongside the Agents SDK for complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you have the agents SDK available\n",
    "# from agents import Agent, Runner, trace\n",
    "\n",
    "async def agents_integration_example():\n",
    "    \"\"\"Integrate FreeLLM with Agents SDK.\"\"\"\n",
    "    async with FreeLLMClient() as client:\n",
    "        # Use FreeLLM for initial brainstorming\n",
    "        print(\"Step 1: Brainstorming with FreeLLM\")\n",
    "        brainstorm_response = await client.chat(\n",
    "            \"Give me 3 creative ideas for a Python project\",\n",
    "            temperature=0.8,\n",
    "        )\n",
    "        display(Markdown(f\"**Ideas from FreeLLM:**\\n\\n{brainstorm_response.response}\"))\n",
    "        \n",
    "        # Then you could pass this to an Agent for further refinement\n",
    "        # Uncomment if you have agents SDK:\n",
    "        # agent = Agent(\n",
    "        #     name=\"project_refiner\",\n",
    "        #     instructions=\"Analyze and refine project ideas\",\n",
    "        #     model=\"gpt-4.1-mini\",\n",
    "        # )\n",
    "        # result = await Runner.run(agent, brainstorm_response.response)\n",
    "        # display(Markdown(result.final_output))\n",
    "        \n",
    "        return brainstorm_response\n",
    "\n",
    "# Run the example\n",
    "response = await agents_integration_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Batch Processing\n",
    "\n",
    "Process multiple messages efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_processing_example():\n",
    "    \"\"\"Process multiple messages.\"\"\"\n",
    "    messages = [\n",
    "        \"What is Python?\",\n",
    "        \"What is JavaScript?\",\n",
    "        \"What is Rust?\",\n",
    "    ]\n",
    "    \n",
    "    async with FreeLLMClient() as client:\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Query {i}: {message}\")\n",
    "            print('='*50)\n",
    "            \n",
    "            response = await client.chat(message, temperature=0.5, max_tokens=100)\n",
    "            display(Markdown(f\"**Response {i}:**\\n\\n{response.response}\"))\n",
    "\n",
    "# Run the example\n",
    "await batch_processing_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Testing the API Connection\n",
    "\n",
    "Test if the API is accessible and responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_api_connection():\n",
    "    \"\"\"Test API connection.\"\"\"\n",
    "    print(\"Testing connection to apifreellm.com...\")\n",
    "    \n",
    "    try:\n",
    "        async with FreeLLMClient(timeout=10.0) as client:\n",
    "            response = await client.chat(\"Hello\")\n",
    "            print(\"✅ Connection successful!\")\n",
    "            print(f\"Response preview: {response.response[:100]}...\")\n",
    "            return True\n",
    "    \n",
    "    except FreeLLMConnectionError:\n",
    "        print(\"❌ Connection failed - Cannot reach API\")\n",
    "        return False\n",
    "    \n",
    "    except FreeLLMAPIError as e:\n",
    "        print(f\"❌ API Error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {type(e).__name__}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "is_connected = await test_api_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Basic chat functionality\n",
    "2. ✅ Custom parameters (temperature, max_tokens)\n",
    "3. ✅ Conversation history and context\n",
    "4. ✅ Error handling with custom exceptions\n",
    "5. ✅ Configuration management\n",
    "6. ✅ Integration possibilities with Agents SDK\n",
    "7. ✅ Batch processing\n",
    "8. ✅ Connection testing\n",
    "\n",
    "The FreeLLM client provides a professional, type-safe interface following OpenAI patterns while being easy to use and integrate into existing workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
