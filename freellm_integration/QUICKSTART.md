# FreeLLM Integration - Quick Start Guide

## ğŸ“ Project Structure

```
freellm_integration/
â”œâ”€â”€ __init__.py              # Package initialization & exports
â”œâ”€â”€ client.py                # Main FreeLLMClient implementation
â”œâ”€â”€ models.py                # Pydantic data models
â”œâ”€â”€ config.py                # Configuration management
â”œâ”€â”€ exceptions.py            # Custom exception hierarchy
â”œâ”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ test_client.py           # Test suite
â”œâ”€â”€ usage_example.ipynb      # Interactive examples
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # Full documentation
â””â”€â”€ QUICKSTART.md           # This file
```

## ğŸš€ Installation

The package uses the following dependencies (already in your project):

```bash
pip install httpx pydantic python-dotenv
```

Or install from requirements:

```bash
cd freellm_integration
pip install -r requirements.txt
```

## ğŸ’¡ Simple Usage

### 1. Basic Example (30 seconds)

```python
import asyncio
from freellm_integration import FreeLLMClient

async def main():
    async with FreeLLMClient() as client:
        response = await client.chat("Hello AI!")
        print(response.response)

asyncio.run(main())
```

### 2. With Jupyter Notebook

```python
from freellm_integration import FreeLLMClient

# In Jupyter, you can use await directly
async with FreeLLMClient() as client:
    response = await client.chat("Hello AI!")
    print(response.response)
```

### 3. One-Liner Test

```python
import asyncio
from freellm_integration import FreeLLMClient

# Test the API in one line
asyncio.run(FreeLLMClient().chat("Test").__await__().__next__().response)
```

## ğŸ¯ Common Use Cases

### Simple Question & Answer

```python
async with FreeLLMClient() as client:
    response = await client.chat("What is Python?")
    print(response.response)
```

### Creative Writing

```python
async with FreeLLMClient() as client:
    response = await client.chat(
        "Write a haiku about coding",
        temperature=0.9  # Higher = more creative
    )
    print(response.response)
```

### Conversation with Memory

```python
async with FreeLLMClient() as client:
    # First message
    r1 = await client.chat_with_context("My name is Alice")

    # Second message (remembers context)
    r2 = await client.chat_with_context("What's my name?")
    print(r2.response)  # Should mention "Alice"
```

### Error Handling

```python
from freellm_integration import FreeLLMAPIError

async with FreeLLMClient() as client:
    try:
        response = await client.chat("Hello")
        print(response.response)
    except FreeLLMAPIError as e:
        print(f"Error: {e}")
```

## ğŸ§ª Testing

Run the test suite:

```bash
cd freellm_integration
python test_client.py
```

## ğŸ“š Examples

### Run the Interactive Notebook

```bash
cd freellm_integration
jupyter notebook usage_example.ipynb
```

The notebook includes 8 complete examples:
1. Basic chat
2. Chat with parameters
3. Conversation with context
4. Error handling
5. Custom configuration
6. Agents SDK integration
7. Batch processing
8. API connection testing

## ğŸ”§ Configuration (Optional)

Create a `.env` file in your project root:

```bash
# Optional: Override defaults
FREELLM_BASE_URL=https://apifreellm.com
FREELLM_TIMEOUT=30.0
FREELLM_MAX_RETRIES=3
FREELLM_DEFAULT_TEMPERATURE=0.7
```

Load configuration:

```python
from freellm_integration.config import FreeLLMConfig

config = FreeLLMConfig.from_env()
client = FreeLLMClient(
    base_url=config.base_url,
    timeout=config.timeout
)
```

## ğŸ—ï¸ Architecture Overview

### Design Patterns Used

1. **Async/Await Pattern**: Modern async Python for efficient I/O
2. **Context Managers**: Proper resource cleanup with `async with`
3. **Dependency Injection**: Configuration can be injected for testing
4. **Single Responsibility**: Each module has one clear purpose
5. **Type Safety**: Pydantic models for runtime validation
6. **Error Hierarchy**: Specific exceptions for different error types

### Key Components

```python
FreeLLMClient        # Main client (like openai.OpenAI)
ChatRequest          # Request model (like OpenAI's messages)
ChatResponse         # Response model (like OpenAI's completion)
FreeLLMConfig        # Configuration (like OpenAI's settings)
FreeLLMError         # Base exception (like OpenAI's APIError)
```

## ğŸ”— Integration Examples

### With Agents SDK

```python
from agents import Agent, Runner
from freellm_integration import FreeLLMClient

async def workflow():
    async with FreeLLMClient() as llm:
        # Step 1: Generate with FreeLLM
        response = await llm.chat("Generate project ideas")

        # Step 2: Process with Agent
        agent = Agent(name="analyzer", instructions="Analyze ideas")
        result = await Runner.run(agent, response.response)

        return result
```

### With MCP Server Pattern

The client follows the same patterns as the MCP examples in `6_mcp/`:

- `accounts_client.py` â†’ Shows MCP client pattern
- `accounts_server.py` â†’ Shows MCP server pattern
- `freellm_integration/` â†’ Shows REST API client pattern

## ğŸ“Š API Reference

### FreeLLMClient

```python
class FreeLLMClient:
    def __init__(
        base_url: str = "https://apifreellm.com",
        timeout: float = 30.0,
        max_retries: int = 3
    )

    async def chat(
        message: str,
        model: str = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> ChatResponse

    async def chat_with_context(
        message: str,
        ...
    ) -> ChatResponse

    def clear_history() -> None
    async def close() -> None
```

## ğŸ†˜ Troubleshooting

### Import Error

```python
# Make sure you're in the right directory
import sys
sys.path.append('..')  # If in a subfolder
```

### Connection Error

```python
# Check if API is accessible
from freellm_integration import FreeLLMClient

async with FreeLLMClient(timeout=10.0) as client:
    response = await client.chat("Test")
    print("âœ… Connected!")
```

### Module Not Found

```bash
# Install dependencies
pip install httpx pydantic python-dotenv
```

## ğŸ“– Next Steps

1. âœ… Run `test_client.py` to verify everything works
2. âœ… Open `usage_example.ipynb` for interactive examples
3. âœ… Read `README.md` for complete documentation
4. âœ… Try integrating with your existing code

## ğŸ¤ Following Best Practices

This implementation follows senior-level software engineering practices:

- âœ… Type hints everywhere
- âœ… Pydantic validation
- âœ… Async/await patterns
- âœ… Context managers
- âœ… Custom exceptions
- âœ… Comprehensive documentation
- âœ… Test coverage
- âœ… Configuration management
- âœ… Clean separation of concerns
- âœ… OpenAI-like API design

---

**Ready to use?** Start with the basic example above or open `usage_example.ipynb`!
